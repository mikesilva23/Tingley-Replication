---
title: "Rough Draft"
author: "Mike Silva"
date: "4/14/2020"
output: pdf_document
bibliography: 1006replicationbib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(foreign)
library(rstanarm)
library(stargazer)
library(generics)
library(tidybayes)
library(broom)
library(modelr)
library(bookdown)

theme_set(theme_tidybayes())
```

# Abstract

Tingley, McDermmott, and Hatemi (2014) find that olfactory senses could explain assortative mating by ideology. My replication of this paper succeeded except for minor discrepancies which do not affect his conclusions. Through my extension, I further evaluate the three models Tingley, McDermott, and Hatemi use by applying a bayesian framework instead of a regular linear model. I further use the sampled iterations created by `stan_lm` to graph density plots of the main coefficient being evaluated in the respective models. Through this framework, I confirm that Tingley's model illustrates little variability in their coefficients. The data and code used in this analysis replication is available on my github. ^[@github]


# Introduction

Have you ever wondered why dogs smell eachother upon meeting eachother? They use smell in deciding whether or not they like each other, and many more animals do this as well. Tingley, McDermmott, and Hatemi take this knowledge to study whether smell affects who we like based on our political ideologies. According to @Iyengar, in 2015, there was a spousal [political] agreement level of 81.5%. There are numerous factors that can attribute to such a high percentage like this, but Tingley, McDermmott, and Hatemi explore the connection to our sense of smell. In order to do so, they surveyed 146 participants asking them to rate the odor of anonymous targets who were either strong liberals or strong conservatives. With this data, they create three separate models. The models all regress the reported attractive rating on numerous variables. In the first model, they primarily focus on the coeficient explaining when the evaluator and the target shared ideologies. The second two models focus on the coefficient that measures the distance between ideology on a 7 point scale. This coeficient explains how a smaller or larger distance apart in ideology between the evaluator and target leads to different attractive ratings. The constant, or the intercept, in these models explain the predicted shift in attractiveness rating of the targets. Through these models, Tingley, McDermmott, and Hatemi observe that their hypotheses hold true even though there were small reported coefficient values. 

For my replication, I use R software^[@R] to translate Tingely's, McDermmott's, and Hatemi's Stata code. Their code is available in the Harvard Dataverse here @Dataverse. As mentioned, the data and code used in this replication is available on my github^[@github]. My replication of this paper was successful except for a few minor differences. I successfully recreated the first two models, but had troubles including target and evaluator fixed effects in the third model. Not including the fixed effects left my model three looking slightly different than the original model. I was also unable to successfully cluster my standard errors on a specific evaluator variable, leaving small discrepancies in the standard errors for the entire table. Regardless of these minor details, my overall replication was a success. 

My extension of the paper focused on further solidifying the significance on Tingley's, McDermmott's, and Hatemi's models. In order to do this, I applied a Bayesian framework to the linear models produced. This allowed me to use the sampled iterations that Bayesian models produce in order to evaluate the distributions of the main coefficients being explained. Interpreting these distributions gives a further understanding of the variability in these predicted values, which helps to explain the significance of the model. If there is little variability, then the model correctly explains the data and holds significance. Through my extension, I found that there was a very tight spread of all three main coefficients being evaluated, which proves that the models are significant. 

# Literature and Paper Review



```{r, warning=FALSE, message=FALSE}
x <- read.dta("AJPSReplication.dta") %>%
  filter(!is.na(attractive)) %>%
  mutate(
    IdeoTargetBinary = ifelse(IdeoTarget >= 4, 1, 0),
    IdeoEvalBinary = ifelse(politicalIdeo >= 4, 1, 0),
    mn_attractive_eval = mean(attractive),
    same_ideology = IdeoTargetBinary == IdeoEvalBinary,
    same_gender = Male == MaleTarget,

    # They appeared to use actual ideology scale instead of binary

    ideodiffabs = abs(politicalIdeo - IdeoTarget)
  )

x <-
  x %>%
  group_by(ideval_n) %>%
  mutate(mn_attractive_eval2 = mean(attractive))
``` 

```{r, include=FALSE}
graphic <-
  x %>%
  sample_n(21, replace = TRUE) %>%
  ggplot(aes(MaleTarget)) + geom_histogram(stat = "count") +
  scale_y_continuous(breaks = c(5, 10)) +
  labs(title = "Target Sex", x = "", y = "Frequency", caption = "This graph illustrates the targeted sexes of the participants. This is taken from a random sample of 21 participants.")

graphic
```



# Appendix

```{r, results='asis'}
model1 <-
  lm(attractive ~ same_ideology + same_gender + IdeoEvalBinary + IdeoTargetBinary + Male + MaleTarget +  mn_attractive + mn_attractive_eval2, data = x)


model2<-
  lm(attractive ~ ideodiffabs + same_gender + politicalIdeo + IdeoTarget + Male + MaleTarget  + mn_attractive + mn_attractive_eval2, data = x)

model3 <-
  lm(attractive ~ ideodiffabs + same_gender, data = x)

labs <- c("Same Ideology", "- Abs Ideology Diff.", "Same Sex", "Conservative Eval.", "Conservative Target","Ideology of Eval.", "Ideology of Target","Male Evaluator","Male Target","Avg. Target Attract","Avg. Eval. Attract")

stargazer(model1, model2, model3, header = FALSE, style = "ajps", type = "latex", omit.stat = c("rsq", "adj.rsq", "ser", "f"), title = "Odor Attraction as a Function of Ideological Similarity", intercept.bottom = T, dep.var.labels.include = F, digits = 4, covariate.labels = labs, dep.var.caption = "")


```




# Extension

Pictured below are individual graphs for each focused parameter in Tingley's three models. These graphs illustrate the distribution of those variables sampled from the model using `stan_lm`. 

```{r Extension,include=FALSE}
set.seed(02364)

stan_fit1 <-
  stan_lm(attractive ~ same_ideology + same_gender + IdeoEvalBinary + IdeoTargetBinary + Male + MaleTarget +  mn_attractive + mn_attractive_eval2, data = x, prior = NULL)


stan1_model <-
  stan_fit1 %>%
  spread_draws(same_ideologyTRUE) %>% 
  ggplot(aes(y = .chain, x = same_ideologyTRUE)) +
  stat_halfeyeh()
 
stan_fit2 <-
  stan_lm(attractive ~ ideodiffabs + same_gender + politicalIdeo + IdeoTarget + Male + MaleTarget  + mn_attractive + mn_attractive_eval2, data = x, prior = NULL)

stan2_model <- 
  stan_fit2 %>%
  spread_draws(ideodiffabs) %>% 
  ggplot(aes(y = .chain, x = ideodiffabs)) +
  stat_halfeyeh()

stan_fit3 <- 
  stan_lm(attractive ~ ideodiffabs + same_gender, data = x, prior = NULL)

stan3_graphic <- 
  stan_fit3 %>% 
  spread_draws(ideodiffabs) %>% 
  ggplot(aes(y = .chain, x = ideodiffabs)) +
  stat_halfeyeh()

```

```{r}
stan1_model
stan2_model
stan3_graphic
```


Through this graph, we can see how well the data explains the outcome we are looking at in our model, which is the measure of attractiveness of the subjects. The dark blue line illustrates the actual outcomes of the model. As you can see, there is a lot of fluctuation between each value. This is because the observers only graded on a whole number scale. The peaks in each hump represent each value- there are 7 peaks that coordinate to each of the 'attractive' values. The faded blue line refers to the posterior iterations ran using pp_check that predicts outcomes using the information in our model. 

Through this graph we can confidently say that our model significantly explains the data. In most cases the predicted values align closesly with the outcome variables, emphasizing how the model is representative of the data. In a the cases of attractive values 1, 4, and 5, the predicted values underestimated compared to the actual values. This may be because there is less data for those values, so the prediction is less accurate. Regardless, this graph confirms the accuracy and significance of the model created by Tingley et al. 




# Selected Bibliography + References:


